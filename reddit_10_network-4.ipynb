{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from string import punctuation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 1000\n",
    "matplotlib.rcParams['lines.linewidth'] = 1.0\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_ngrams(tokens, rg):\n",
    "    ngrms = []\n",
    "    for i in range(rg[0], rg[1] + 1):\n",
    "        ngrms_aux = [ngrm for ngrm in ngrams(tokens, i)]\n",
    "        ngrms.extend(ngrms_aux)\n",
    "    return ngrms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_keywords = [\"Picard\", \"Kirk\", \"Sulu\", \"Uhura\", \"Spock\", \"McCoy\", \"Bones\", \"Scotty\", \"Chekhov\", \"Crusher\", \"Nurse Chapel\", \"Sarek\", \"Nero\", \"Khan\", \"Data\", \"Pike\", \"Star Trek\", \"Klingon\", \"Vulcan\", \"Romulan\", \"Star fleet\", \"Starship Enterprise\", \"Delta Vega\", \"Earth\", \"Orion\", \"Romulan Narada\", \"Stardate\", \"Transporter beam\", \"Beam me up,\\? Scotty\", \"Kobayashi Maru\", \"Space,\\? The Final Frontier\", \"Energize\", \"Vulcan Salute\", \"Prime Directive\", \"Live long and prosper\", \"LLAP\", \"I'm a doctor,\\? not a\", \"KHAAA\\+N\", \"When you eliminate the impossible,\\? whatever remains,\\? however improbable,\\? must be the truth\", \"Without followers,\\? evil cannot spread\", \"The needs of the many outweigh the needs of the few\", \"Highly illogical\", \"to boldly go where no man has gone before\", \"I'm giving her all she's got,\\? Captain\", \"Nuclear wessels\", \"Set phasers to stun\", \"Resistance is futile\", \"I have been and always shall be your friend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Picard', 'Kirk', 'Sulu', 'Uhura', 'Spock', 'McCoy', 'Bones', 'Scotty', 'Chekhov', 'Crusher', 'Nurse Chapel', 'Sarek', 'Nero', 'Khan', 'Data', 'Pike', 'Star Trek', 'Klingon', 'Vulcan', 'Romulan', 'Star fleet', 'Starship Enterprise', 'Delta Vega', 'Earth', 'Orion', 'Romulan Narada', 'Stardate', 'Transporter beam', 'Beam me up,\\\\? Scotty', 'Kobayashi Maru', 'Space,\\\\? The Final Frontier', 'Energize', 'Vulcan Salute', 'Prime Directive', 'Live long and prosper', 'LLAP', \"I'm a doctor,\\\\? not a\", 'KHAAA\\\\+N', 'When you eliminate the impossible,\\\\? whatever remains,\\\\? however improbable,\\\\? must be the truth', 'Without followers,\\\\? evil cannot spread', 'The needs of the many outweigh the needs of the few', 'Highly illogical', 'to boldly go where no man has gone before', \"I'm giving her all she's got,\\\\? Captain\", 'Nuclear wessels', 'Set phasers to stun', 'Resistance is futile', 'I have been and always shall be your friend']\n"
     ]
    }
   ],
   "source": [
    "print(original_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ambiguous_keywords = [\n",
    "    'Bones',\n",
    "    'Khan',\n",
    "    'Data',\n",
    "    'Earth',\n",
    "    'Energize',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = list(set(original_keywords).difference(ambiguous_keywords))\n",
    "keywords = [w.replace('\\\\', '').lower() for w in keywords]\n",
    "keywords.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beam me up,? scotty', 'chekhov', 'crusher', 'delta vega', 'highly illogical', 'i have been and always shall be your friend', \"i'm a doctor,? not a\", \"i'm giving her all she's got,? captain\", 'khaaa+n', 'kirk', 'klingon', 'kobayashi maru', 'live long and prosper', 'llap', 'mccoy', 'nero', 'nuclear wessels', 'nurse chapel', 'orion', 'picard', 'pike', 'prime directive', 'resistance is futile', 'romulan', 'romulan narada', 'sarek', 'scotty', 'set phasers to stun', 'space,? the final frontier', 'spock', 'star fleet', 'star trek', 'stardate', 'starship enterprise', 'sulu', 'the needs of the many outweigh the needs of the few', 'to boldly go where no man has gone before', 'transporter beam', 'uhura', 'vulcan', 'vulcan salute', 'when you eliminate the impossible,? whatever remains,? however improbable,? must be the truth', 'without followers,? evil cannot spread']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 s, sys: 2.18 s, total: 32.5 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dtypes = {\n",
    "    'author': str,\n",
    "    'body': str,\n",
    "    'controversiality': int,\n",
    "    'created_utc': pd.tslib.Timestamp,\n",
    "    'distinguished': str,\n",
    "    'downs': str,\n",
    "    'gilded': str,\n",
    "    'id': str,\n",
    "    'name': str,\n",
    "    'parent_id': str,\n",
    "    'score': int,\n",
    "    'subreddit': str,\n",
    "    'ups': str,\n",
    "    'month': pd.tslib.Timestamp,\n",
    "    'year': pd.tslib.Timestamp,\n",
    "}\n",
    "\n",
    "reddit_df = pd.read_csv('reddit.csv', header=0, dtype=dtypes, parse_dates=['created_utc', 'month', 'year'])\n",
    "reddit_df = reddit_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669814"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gschizas</td>\n",
       "      <td>same in greek. my personal peeve: quite a lot ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-15 15:22:33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c029c79</td>\n",
       "      <td>t1_c029c79</td>\n",
       "      <td>t1_c029brz</td>\n",
       "      <td>13</td>\n",
       "      <td>programming</td>\n",
       "      <td>13</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>&amp;gt;they also decided to rig a thermal barrier...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-16 05:11:09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c029g6h</td>\n",
       "      <td>t1_c029g6h</td>\n",
       "      <td>t3_5ye94</td>\n",
       "      <td>2</td>\n",
       "      <td>programming</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>johnmudd</td>\n",
       "      <td>&amp;gt; by the time a child born today graduates ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-16 07:00:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c029gfc</td>\n",
       "      <td>t1_c029gfc</td>\n",
       "      <td>t3_5ye9w</td>\n",
       "      <td>1</td>\n",
       "      <td>science</td>\n",
       "      <td>1</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feliniti</td>\n",
       "      <td>nero 8 - bloated waste of money. unless you ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-16 07:10:27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c029gge</td>\n",
       "      <td>t1_c029gge</td>\n",
       "      <td>t3_5yej4</td>\n",
       "      <td>1</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>masklinn</td>\n",
       "      <td>&amp;gt; i wonder what happens if you started clea...</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-16 11:27:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c029hiy</td>\n",
       "      <td>t1_c029hiy</td>\n",
       "      <td>t1_c029h3y</td>\n",
       "      <td>3</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>3</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>2007-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                               body  \\\n",
       "0   gschizas  same in greek. my personal peeve: quite a lot ...   \n",
       "1  [deleted]  &gt;they also decided to rig a thermal barrier...   \n",
       "2   johnmudd  &gt; by the time a child born today graduates ...   \n",
       "3   feliniti  nero 8 - bloated waste of money. unless you ha...   \n",
       "4   masklinn  &gt; i wonder what happens if you started clea...   \n",
       "\n",
       "   controversiality         created_utc distinguished downs gilded       id  \\\n",
       "0                 0 2007-10-15 15:22:33             0   0.0      0  c029c79   \n",
       "1                 0 2007-10-16 05:11:09             0   0.0      0  c029g6h   \n",
       "2                 0 2007-10-16 07:00:36             0   0.0      0  c029gfc   \n",
       "3                 0 2007-10-16 07:10:27             0   0.0      0  c029gge   \n",
       "4                 0 2007-10-16 11:27:15             0   0.0      0  c029hiy   \n",
       "\n",
       "         name   parent_id  score    subreddit ups      month       year  \n",
       "0  t1_c029c79  t1_c029brz     13  programming  13 2007-10-01 2007-01-01  \n",
       "1  t1_c029g6h    t3_5ye94      2  programming   2 2007-10-01 2007-01-01  \n",
       "2  t1_c029gfc    t3_5ye9w      1      science   1 2007-10-01 2007-01-01  \n",
       "3  t1_c029gge    t3_5yej4      1   reddit.com   1 2007-10-01 2007-01-01  \n",
       "4  t1_c029hiy  t1_c029h3y      3   reddit.com   3 2007-10-01 2007-01-01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = nx.read_gexf('user_graph_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.number_of_selfloops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_users = [d['show_label'] for n, d in graph.nodes_iter(data=True) if 'show_label' in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634029"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.author.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_reddit_df = reddit_df[reddit_df.author.isin(top_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152383"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_reddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669814"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024% of users post 5.71% of comments\n"
     ]
    }
   ],
   "source": [
    "a = 100 * len(top_users) / reddit_df.author.nunique()\n",
    "b = 100 * len(top_reddit_df) / len(reddit_df)\n",
    "print('%.3f%% of users post %.2f%% of comments' % (round(a, 3), round(b, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Calcular graficas, histogramas y sentiment analysis de top_reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(graph, 'user_graph_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
